import cv2
import numpy as np
import pyaudio
import time
import threading
from ultralytics import YOLO

# --- è¨­å®šåƒæ•¸ ---
THRESHOLD_DB = 30        # è§¸ç™¼åˆ†è²
CHUNK = 1024             # éŸ³è¨Šç·©è¡å€å¤§å°
FORMAT = pyaudio.paInt16 # éŸ³è¨Šæ ¼å¼
CHANNELS = 1             # å–®è²é“
RATE = 44100             # å–æ¨£ç‡
COOLDOWN = 3             # æ‹ç…§å†·å»æ™‚é–“ (ç§’)

# --- åˆå§‹åŒ– ---
print("æ­£åœ¨è¼‰å…¥ AI æ¨¡å‹...")
model = YOLO('yolov8n.pt') 
audio = pyaudio.PyAudio()
cap = cv2.VideoCapture(0)  

# è¨­å®šå­—é«”
FONT = cv2.FONT_HERSHEY_SIMPLEX

def calculate_db(audio_data):
    # å°‡äºŒé€²ä½æ•¸æ“šè½‰æ›ç‚ºæ•´æ•¸é™£åˆ—
    data = np.frombuffer(audio_data, dtype=np.int16)
    # è¨ˆç®— RMS (å‡æ–¹æ ¹)
    rms = np.sqrt(np.mean(data**2))
    # è½‰æ›ç‚ºåˆ†è² (éœ€ä¾éº¥å…‹é¢¨éˆæ•åº¦æ ¡æ­£ï¼Œé€™è£¡åŠ  20 æ˜¯ç‚ºäº†è®“æ•¸å€¼å¥½çœ‹ä¸€é»)
    if rms > 0:
        db = 20 * np.log10(rms) + 20 
    else:
        db = 0
    return db

def detect_and_save(frame, current_db):
    print(f"ğŸ”Š è§¸ç™¼ï¼({current_db:.1f} dB) æ­£åœ¨åˆ†æ...")
    results = model(frame)
    
    # é€™è£¡æ”¹æˆ 0 (äºº) æ–¹ä¾¿æ‚¨æ¸¬è©¦ï¼Œè‹¥è¦æŠ“è»Šæ”¹å› [2, 3, 5, 7]
    target_classes = [ 2, 3, 5, 7, 39, 41, 67] 
    
    vehicle_detected = False
    
    for r in results:
        for box in r.boxes:
            cls_id = int(box.cls[0])
            if cls_id in target_classes:
                vehicle_detected = True
                # ç•«å‡ºç‰©ä»¶æ¡†
                x1, y1, x2, y2 = map(int, box.xyxy[0])
                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), 2)
                label = f"{model.names[cls_id]} {box.conf[0]:.2f}"
                cv2.putText(frame, label, (x1, y1 - 10), FONT, 0.5, (0, 0, 255), 2)

    if vehicle_detected:
        filename = f"capture_{int(time.time())}.jpg"
        cv2.imwrite(filename, frame)
        print(f"ğŸ“¸ å·²å­˜æª”: {filename}")

def main_loop():
    last_trigger_time = 0
    
    stream = audio.open(format=FORMAT, channels=CHANNELS,
                        rate=RATE, input=True,
                        frames_per_buffer=CHUNK)

    print(f"ç³»çµ±å•Ÿå‹•ä¸­... å°è‘—éº¥å…‹é¢¨å¤§å«è©¦è©¦çœ‹ï¼")

    try:
        while True:
            # 1. è®€å–ç•«é¢
            ret, frame = cap.read()
            if not ret: break

            # 2. è®€å–è²éŸ³ä¸¦è¨ˆç®—åˆ†è²
            data = stream.read(CHUNK, exception_on_overflow=False)
            db = calculate_db(data)

            # --- [æ–°å¢åŠŸèƒ½] æ±ºå®šæ–‡å­—é¡è‰² ---
            if db > THRESHOLD_DB:
                text_color = (0, 0, 255) # ç´…è‰² (è­¦å‘Š)
                status_text = "WARNING!"
            else:
                text_color = (0, 255, 0) # ç¶ è‰² (æ­£å¸¸)
                status_text = "Normal"

            # --- [æ–°å¢åŠŸèƒ½] å°‡åˆ†è²æ•¸å¯«åœ¨ç•«é¢ä¸Š ---
            # åƒæ•¸æ ¼å¼: å½±åƒ, æ–‡å­—, åº§æ¨™(x,y), å­—é«”, å¤§å°, é¡è‰², ç²—ç´°
            cv2.putText(frame, f"Noise Level: {int(db)} dB", (30, 50), 
                       FONT, 1.2, text_color, 3)
            
            # é¡¯ç¤ºç‹€æ…‹æ–‡å­—
            cv2.putText(frame, f"Status: {status_text}", (30, 100), 
                       FONT, 0.8, text_color, 2)

            # 3. åˆ¤æ–·è§¸ç™¼ (èˆ‡ä¹‹å‰é‚è¼¯ç›¸åŒ)
            current_time = time.time()
            if db > THRESHOLD_DB and (current_time - last_trigger_time) > COOLDOWN:
                last_trigger_time = current_time
                # å‚³å…¥ frame.copy() é¿å…ç¹ªåœ–å½±éŸ¿åˆ°åŸå§‹å½±åƒåˆ†æ
                threading.Thread(target=detect_and_save, args=(frame.copy(), db)).start()

            # 4. é¡¯ç¤ºè¦–çª—
            cv2.imshow('AI Noise Camera', frame)

            if cv2.waitKey(1) & 0xFF == ord('q'):
                break

    except KeyboardInterrupt:
        print("åœæ­¢ç¨‹å¼...")
    finally:
        stream.stop_stream()
        stream.close()
        audio.terminate()
        cap.release()
        cv2.destroyAllWindows()

if __name__ == "__main__":
    main_loop()